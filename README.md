# Transformers-Text-Classification
Suggesting a neural network architecture for analyzing and recognizing texts, where transformers were used through a pre-trained BERT model, in addition to its integration with the LSTM layer with the Global Pooling layers, in order to reach a model capable of analyzing texts.
# Result:
## Neural Network Architecture:

![image](https://user-images.githubusercontent.com/108609519/184367055-1e1bb9c4-b5eb-446a-97f1-356ff0d90b73.png)

## Metrics:
### Accuracy, Recall, Precision:
![image](https://user-images.githubusercontent.com/108609519/184367276-b61f256b-3004-498f-b38e-cdb0c3723430.png)
### Loss While Training:
![image](https://user-images.githubusercontent.com/108609519/184368689-8afa080e-c96a-4833-9b1e-932ecb965078.png)

## Evaluate with Validation Data:

![image](https://user-images.githubusercontent.com/108609519/184368509-239d06e2-f071-4e45-b9a4-1fd50a7eaea4.png)


